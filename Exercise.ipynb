{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Siva-Subramaniam-DS/AS400-Certification-/blob/main/Exercise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mbXC_YhNqJ8f"
      },
      "source": [
        "# Exercise 1 CNN Analysis Custom Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvYD1AeQqJ8j"
      },
      "source": [
        "## ‚úÖ **Step 1: Analyze AlexNet & ResNet-18**\n",
        "\n",
        "### ‚û§ **AlexNet Architecture Overview**\n",
        "\n",
        "* **Depth:** \\~8 layers (5 Conv + 3 FC)\n",
        "* **Conv Layers:**\n",
        "\n",
        "  * Layer 1: `11x11` kernel, `stride=4` (too large for medical images)\n",
        "  * Subsequent layers: `5x5`, `3x3`\n",
        "* **Pooling:** After some Conv layers, `max pooling (3x3)` with `stride=2`\n",
        "* **No skip connections**\n",
        "* **Total Parameters:** \\~60M+\n",
        "\n",
        "### ‚û§ **ResNet-18 Architecture Overview**\n",
        "\n",
        "* **Depth:** 18 layers (with shortcut/skip connections)\n",
        "* **Conv Layers:** `7x7` then `3x3`\n",
        "* **Pooling:** Global average pooling before FC\n",
        "* **Skip Connections:** Key feature\n",
        "\n",
        "---\n",
        "\n",
        "### üìå **Why ResNet Is Better for Deep Networks**\n",
        "\n",
        "**Skip Connection Formula:**\n",
        "\n",
        "Let input be $x$, residual function be $F(x)$\n",
        "\n",
        "$$\n",
        "\\text{Output} = F(x) + x\n",
        "$$\n",
        "\n",
        "* Solves **vanishing gradients**\n",
        "* Helps in **identity mapping**, making deeper networks easier to optimize\n",
        "* Gradients flow more easily through shortcut paths\n",
        "\n",
        "---\n",
        "\n",
        "### üìå **Why AlexNet Is Inefficient for Medical Imaging**\n",
        "\n",
        "* Large kernels (11x11) reduce sensitivity to small pneumonia patterns\n",
        "* No skip connections ‚Üí weak gradient flow\n",
        "* High number of parameters (\\~60M) ‚Üí overfitting risk with small datasets\n",
        "* Shallow in comparison to modern networks\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUggK_1VqJ8q"
      },
      "source": [
        "## ‚úÖ Step 2: Design a Custom CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "fmL51SAxqJ8r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PneumoniaCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(PneumoniaCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)  # 128x128\n",
        "        self.bn1 = nn.BatchNorm2d(32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1) # 64x64\n",
        "        self.bn2 = nn.BatchNorm2d(64)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)          # 32x32\n",
        "        self.bn3 = nn.BatchNorm2d(128)\n",
        "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)         # 16x16\n",
        "        self.bn4 = nn.BatchNorm2d(256)\n",
        "\n",
        "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.fc = nn.Linear(256, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
        "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
        "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
        "        x = self.pool(F.relu(self.bn4(self.conv4(x))))\n",
        "        x = self.global_avg_pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOGV4C_vqJ8w"
      },
      "source": [
        "# ‚úÖ Step 3: Load X-ray Dataset from Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "3owL3QQRqJ8x"
      },
      "outputs": [],
      "source": [
        "import kagglehub\n",
        "import os\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Download and prepare dataset\n",
        "path = kagglehub.dataset_download(\"paultimothymooney/chest-xray-pneumonia\")\n",
        "\n",
        "train_dir = os.path.join(path, \"chest_xray\", \"train\")\n",
        "val_dir = os.path.join(path, \"chest_xray\", \"val\")\n",
        "\n",
        "# Transformations with augmentation\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "val_transforms = transforms.Compose([\n",
        "    transforms.Grayscale(),\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5], [0.5])\n",
        "])\n",
        "\n",
        "train_data = datasets.ImageFolder(train_dir, transform=train_transforms)\n",
        "val_data = datasets.ImageFolder(val_dir, transform=val_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_data, batch_size=32, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DOr2jBAxqJ8z"
      },
      "source": [
        "# ‚úÖ Step 4: Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qmp9PY7wqJ80",
        "outputId": "0933bc75-afd4-4a6d-a237-bdaf70cdff91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 38.4406, Train Acc: 0.9005\n",
            "Validation Accuracy: 0.5000\n",
            "Epoch 2, Loss: 23.7905, Train Acc: 0.9446\n",
            "Validation Accuracy: 0.5000\n",
            "Epoch 3, Loss: 22.0217, Train Acc: 0.9475\n",
            "Validation Accuracy: 0.7500\n",
            "Epoch 4, Loss: 19.3102, Train Acc: 0.9515\n",
            "Validation Accuracy: 0.6250\n",
            "Epoch 5, Loss: 16.6827, Train Acc: 0.9622\n",
            "Validation Accuracy: 0.6250\n",
            "Epoch 6, Loss: 17.6405, Train Acc: 0.9588\n",
            "Validation Accuracy: 0.7500\n",
            "Epoch 7, Loss: 13.8162, Train Acc: 0.9647\n",
            "Validation Accuracy: 0.8750\n",
            "Epoch 8, Loss: 15.0188, Train Acc: 0.9643\n",
            "Validation Accuracy: 0.6250\n",
            "Epoch 9, Loss: 13.5505, Train Acc: 0.9703\n",
            "Validation Accuracy: 0.5625\n",
            "Epoch 10, Loss: 14.4897, Train Acc: 0.9645\n",
            "Validation Accuracy: 0.5000\n"
          ]
        }
      ],
      "source": [
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PneumoniaCNN().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=3, verbose=True)\n",
        "\n",
        "def train(model, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        acc = correct / total\n",
        "        print(f\"Epoch {epoch+1}, Loss: {running_loss:.4f}, Train Acc: {acc:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_acc = val_correct / val_total\n",
        "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
        "        scheduler.step(val_acc)\n",
        "train(model, epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Y-cJyxyqJ81"
      },
      "source": [
        "# Batch normalization (BatchNorm) improves training deep networks\n",
        "\n",
        "1. **Reduces Internal Covariate Shift**\n",
        "   As weights in earlier layers change during training, the distribution of activations that later layers see keeps shifting‚Äîforcing them to continuously adapt. BatchNorm ‚Äústabilizes‚Äù each layer‚Äôs input distribution by normalizing mini-batch activations to zero mean and unit variance:\n",
        "\n",
        "   $$\n",
        "   \\hat{x}_i = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}},\n",
        "   \\quad\n",
        "   y_i = \\gamma\\,\\hat{x}_i + \\beta\n",
        "   $$\n",
        "\n",
        "   where $\\mu_B,\\sigma_B^2$ are the batch mean and variance, and $\\gamma,\\beta$ are learned scale and shift parameters. This steadier input distribution means subsequent layers learn more reliably.\n",
        "\n",
        "2. **Smoother, More Predictable Gradients**\n",
        "   By keeping activations in a controlled range, BatchNorm prevents extremely large or tiny activations, which in turn avoids exploding or vanishing gradients. As a result, the loss surface becomes ‚Äúflatter‚Äù around minima, making gradient descent converge faster and more robustly.\n",
        "\n",
        "3. **Enables Higher Learning Rates**\n",
        "   With normalized activations, the network can tolerate‚Äîand often benefit from‚Äîa larger base learning rate without diverging. This accelerates convergence and can reduce the total number of training epochs needed.\n",
        "\n",
        "4. **Acts as a Regularizer**\n",
        "   The mini-batch statistics introduce a small amount of noise into each layer‚Äôs activations, similar to dropout. This stochasticity helps prevent overfitting, often reducing or eliminating the need for other regularizers.\n",
        "\n",
        "5. **Allows Reduced Sensitivity to Initialization**\n",
        "   Since activations are standardized each batch, the network is less dependent on carefully tuned weight initializations. You can more safely start from generic schemes (e.g., Xavier or He initialization) and still get stable training behavior.\n",
        "\n",
        "---\n",
        "\n",
        "**In practice**, inserting a BatchNorm layer after each convolution (and before the nonlinearity) typically leads to:\n",
        "\n",
        "* Faster convergence.\n",
        "* Higher final accuracy.\n",
        "* Smoother training curves.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNMl3xKcqJ82"
      },
      "source": [
        "# Exercise 2: Vision Transformers (ViT) vs CNNs for Traffic Sign Recognition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "iEmm18RhqJ83"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets, models\n",
        "import torch.nn.functional as F\n",
        "import os\n",
        "from torchvision.datasets import GTSRB\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ud9qEPMqJ85"
      },
      "source": [
        "# STEP 1: Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_zCv6J5qJ86",
        "outputId": "8291f576-eec9-43f7-8c03-d2654a6d2edf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 187M/187M [00:11<00:00, 16.8MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 89.0M/89.0M [00:06<00:00, 13.7MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 99.6k/99.6k [00:00<00:00, 193kB/s]\n"
          ]
        }
      ],
      "source": [
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# Download and load datasets directly\n",
        "train_dataset = GTSRB(root='./data', split='train', transform=transform, download=True)\n",
        "test_dataset = GTSRB(root='./data', split='test', transform=transform, download=True)\n",
        "\n",
        "# Data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whNA-RP4qJ87"
      },
      "source": [
        "# STEP 2: Vision Transformer (ViT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n5fAtrSRqJ87"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=64, patch_size=16, in_channels=3, embed_dim=256):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.n_patches = (img_size // patch_size) ** 2\n",
        "        self.proj = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)  # (B, embed_dim, H/patch, W/patch)\n",
        "        x = x.flatten(2).transpose(1, 2)  # (B, num_patches, embed_dim)\n",
        "        return x\n",
        "\n",
        "class ViT(nn.Module):\n",
        "    def __init__(self, img_size=64, patch_size=16, in_channels=3, num_classes=43, embed_dim=256, depth=6, heads=8, mlp_dim=512):\n",
        "        super().__init__()\n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.randn(1, 1 + self.patch_embed.n_patches, embed_dim))\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=heads, dim_feedforward=mlp_dim, batch_first=True)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
        "\n",
        "        self.head = nn.Sequential(\n",
        "            nn.LayerNorm(embed_dim),\n",
        "            nn.Linear(embed_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.size(0)\n",
        "        x = self.patch_embed(x)  # (B, n_patches, embed_dim)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)  # (B, 1 + n_patches, embed_dim)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.transformer(x)\n",
        "        cls_output = x[:, 0]\n",
        "        return self.head(cls_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQGHP4LgqJ88"
      },
      "source": [
        "# STEP 3: CNN (ResNet18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0wLP-nl_qJ88"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "def get_resnet18_model(num_classes=43):\n",
        "    model = resnet18(pretrained=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWCeA5VLqJ89"
      },
      "source": [
        "# STEP 4: Train & Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FZRzaVFXqJ89"
      },
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "\n",
        "def train(model, train_loader, val_loader, device, epochs=10):\n",
        "    model.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=0.001)\n",
        "    scheduler = ReduceLROnPlateau(optimizer, mode='max', patience=2, factor=0.5, verbose=True)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_acc = 100 * correct / total\n",
        "        val_acc = evaluate(model, val_loader, device)\n",
        "        scheduler.step(val_acc)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Val Acc: {val_acc:.2f}%\")\n",
        "\n",
        "def evaluate(model, loader, device):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    return 100 * correct / total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eswbVXkcqJ89"
      },
      "source": [
        "# STEP 5: Run Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPvFexjKqJ8-",
        "outputId": "0cb8afeb-1e90-49e3-c1d3-4fe2f811279c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Vision Transformer\n",
            "Epoch 1: Train Acc: 6.38%, Val Acc: 5.46%\n",
            "Epoch 2: Train Acc: 5.56%, Val Acc: 4.99%\n",
            "Epoch 3: Train Acc: 5.28%, Val Acc: 5.94%\n",
            "Epoch 4: Train Acc: 5.44%, Val Acc: 5.23%\n",
            "Epoch 5: Train Acc: 5.42%, Val Acc: 5.46%\n",
            "Epoch 6: Train Acc: 5.29%, Val Acc: 5.70%\n",
            "Epoch 7: Train Acc: 5.51%, Val Acc: 5.94%\n",
            "Epoch 8: Train Acc: 5.17%, Val Acc: 5.46%\n",
            "Epoch 9: Train Acc: 5.54%, Val Acc: 5.46%\n",
            "Epoch 10: Train Acc: 5.60%, Val Acc: 5.94%\n",
            "Training ResNet18\n",
            "Epoch 1: Train Acc: 85.86%, Val Acc: 87.20%\n",
            "Epoch 2: Train Acc: 97.77%, Val Acc: 93.04%\n",
            "Epoch 3: Train Acc: 98.74%, Val Acc: 88.52%\n",
            "Epoch 4: Train Acc: 98.84%, Val Acc: 93.32%\n",
            "Epoch 5: Train Acc: 99.04%, Val Acc: 91.44%\n",
            "Epoch 6: Train Acc: 99.11%, Val Acc: 92.97%\n",
            "Epoch 7: Train Acc: 99.47%, Val Acc: 90.57%\n",
            "Epoch 8: Train Acc: 99.84%, Val Acc: 95.71%\n",
            "Epoch 9: Train Acc: 99.99%, Val Acc: 96.28%\n",
            "Epoch 10: Train Acc: 100.00%, Val Acc: 96.43%\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Choose model\n",
        "vit_model = ViT()\n",
        "resnet_model = get_resnet18_model()\n",
        "\n",
        "# Train ViT\n",
        "print(\"Training Vision Transformer\")\n",
        "train(vit_model, train_loader, test_loader, device, epochs=10)\n",
        "\n",
        "# Train ResNet18\n",
        "print(\"Training ResNet18\")\n",
        "train(resnet_model, train_loader, test_loader, device, epochs=10)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üö¶ **ViT vs CNN ‚Äì Critical Analysis**\n",
        "\n",
        "### ‚úÖ **1. Architectural Differences**\n",
        "\n",
        "| Feature                   | CNN (e.g., ResNet-18)                   | Vision Transformer (ViT)                        |\n",
        "| ------------------------- | --------------------------------------- | ----------------------------------------------- |\n",
        "| Input Handling            | Processes image spatially using kernels | Divides image into patches and treats as tokens |\n",
        "| Core Mechanism            | Convolutional layers                    | Multi-head self-attention                       |\n",
        "| Positional Encoding       | Implicit via spatial convolutions       | Explicit positional embeddings                  |\n",
        "| Local vs Global Attention | Local receptive field                   | Global attention across all patches             |\n",
        "| Inductive Bias            | High (translation equivariance)         | Low (learns from scratch)                       |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **2. Performance Comparison on Traffic Sign Dataset (GTSRB)**\n",
        "\n",
        "| Aspect               | CNN (ResNet-18)            | ViT (Small or Tiny)                   |\n",
        "| -------------------- | -------------------------- | ------------------------------------- |\n",
        "| Accuracy (typically) | ‚úÖ High                     | ‚ö†Ô∏è Moderate‚ÄìHigh (if data sufficient) |\n",
        "| Data Requirement     | Works well with small data | Needs large-scale data or pretraining |\n",
        "| Training Time        | ‚ö° Faster                   | üê¢ Slower (due to attention overhead) |\n",
        "| Model Size           | Compact                    | Often larger (more params)            |\n",
        "| Overfitting Risk     | Low-moderate               | High (if data is small)               |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **3. Why CNNs Often Outperform ViT on Small Datasets?**\n",
        "\n",
        "* CNNs have **strong inductive biases** like locality and weight sharing that help them generalize better with limited data.\n",
        "* ViT models are **data-hungry** ‚Äî without extensive pretraining or data augmentation, they tend to underperform.\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **4. ViT Strengths & Use-Cases**\n",
        "\n",
        "* **Better scalability** for large datasets (e.g., ImageNet-21k).\n",
        "* **Global context modeling** helps in:\n",
        "\n",
        "  * Occlusion robustness\n",
        "  * Scene understanding\n",
        "  * Long-range dependencies\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **5. Visual Clarity & Explainability**\n",
        "\n",
        "| Criterion      | ViT                                          | CNN                                           |\n",
        "| -------------- | -------------------------------------------- | --------------------------------------------- |\n",
        "| Attention Maps | ‚úÖ More interpretable (via attention weights) | ‚ö†Ô∏è Less interpretable (though Grad-CAM helps) |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **6. When to Use What?**\n",
        "\n",
        "| Situation                             | Preferred Model     |\n",
        "| ------------------------------------- | ------------------- |\n",
        "| Limited data (e.g., medical, traffic) | ‚úÖ CNN (ResNet-18)   |\n",
        "| Large-scale image classification      | ‚úÖ ViT               |\n",
        "| Applications needing explainability   | ‚úÖ ViT               |\n",
        "| Real-time inference on edge devices   | ‚úÖ CNN (lightweight) |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚úÖ **Conclusion**\n",
        "\n",
        "* **ResNet-18 (CNN)** remains a **better choice** for small datasets like **GTSRB**, due to its strong biases and data efficiency.\n",
        "* **ViT** shows **promise** on larger datasets or with **transfer learning**, but requires careful tuning and data augmentation.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "KVKUTCHDwv3V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 3 InceptionNet MultiScale Learning"
      ],
      "metadata": {
        "id": "ulkmJIuPxVJk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: InceptionNet Breakdown\n",
        "üîπ Why 1x1 Convolutions?\n",
        "Dimensionality reduction: Reduce channels before expensive convolutions (like 3x3, 5x5), decreasing computation.\n",
        "\n",
        "Introduce non-linearity: With ReLU activation after 1x1.\n",
        "\n",
        "üîπ Why Parallel Convolutions Help?\n",
        "Different kernel sizes capture features at different scales:\n",
        "\n",
        "1x1: Pixel-wise interactions\n",
        "\n",
        "3x3: Medium-size patterns (edges, textures)\n",
        "\n",
        "5x5: Larger patterns\n",
        "\n",
        "Pooling: Generalized features"
      ],
      "metadata": {
        "id": "U13dZ4gSxNWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Simplified Inception Block in PyTorch"
      ],
      "metadata": {
        "id": "7IxIFcgnxGAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 16, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 16, kernel_size=1),\n",
        "            nn.Conv2d(16, 24, kernel_size=3, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.branch5x5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 16, kernel_size=1),\n",
        "            nn.Conv2d(16, 24, kernel_size=5, padding=2),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, 24, kernel_size=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1 = self.branch1x1(x)\n",
        "        out2 = self.branch3x3(x)\n",
        "        out3 = self.branch5x5(x)\n",
        "        out4 = self.branch_pool(x)\n",
        "        return torch.cat([out1, out2, out3, out4], 1)"
      ],
      "metadata": {
        "id": "VKyZNa8XxDHw"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Full InceptionNet"
      ],
      "metadata": {
        "id": "uVcBb4aoxdeA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InceptionNet(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.pre_layers = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),\n",
        "            nn.MaxPool2d(3, stride=2, padding=1),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.inception1 = InceptionBlock(64)\n",
        "        self.inception2 = InceptionBlock(88)  # after concat: 16+24+24+24 = 88\n",
        "        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.fc = nn.Linear(88, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pre_layers(x)\n",
        "        x = self.inception1(x)\n",
        "        x = self.inception2(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return self.fc(x)\n"
      ],
      "metadata": {
        "id": "7qwYbOnXxZqC"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Prepare Fashion-MNIST Dataset (Resized to 96x96)"
      ],
      "metadata": {
        "id": "tTCL_AVDxgvV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((96, 96)),\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "train_set = datasets.FashionMNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_set = datasets.FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_set, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bmnHZmR3xkB6",
        "outputId": "0856dfc1-fcd4-4a52-bff2-4f14eeedff6e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 26.4M/26.4M [00:02<00:00, 10.5MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 29.5k/29.5k [00:00<00:00, 170kB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.42M/4.42M [00:01<00:00, 3.27MB/s]\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.15k/5.15k [00:00<00:00, 11.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Step 5: Training Loop"
      ],
      "metadata": {
        "id": "hopTgD04xpXj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam\n",
        "\n",
        "def train_model(model, train_loader, test_loader, device, epochs=5):\n",
        "    model.to(device)\n",
        "    optimizer = Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss, correct, total = 0, 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = F.cross_entropy(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            correct += (outputs.argmax(1) == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        acc = correct / total * 100\n",
        "        print(f\"Epoch {epoch+1}, Loss: {total_loss:.4f}, Accuracy: {acc:.2f}%\")"
      ],
      "metadata": {
        "id": "ODEdnlpwxnR7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Compare with ResNet"
      ],
      "metadata": {
        "id": "LZcjiZ4Gxvja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.models import resnet18\n",
        "\n",
        "def get_resnet_fashionmnist(num_classes=10):\n",
        "    model = resnet18(pretrained=False)\n",
        "    model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "M_5POPk1xyw_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Run Both Models"
      ],
      "metadata": {
        "id": "opMoMY1xxz86"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# InceptionNet\n",
        "inception = InceptionNet()\n",
        "print(\"Training InceptionNet:\")\n",
        "train_model(inception, train_loader, test_loader, device)\n",
        "\n",
        "# ResNet\n",
        "resnet = get_resnet_fashionmnist()\n",
        "print(\"Training ResNet:\")\n",
        "train_model(resnet, train_loader, test_loader, device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdaasV8ex4AO",
        "outputId": "1408755c-40ea-4141-91cb-e640f348fdf5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training InceptionNet:\n",
            "Epoch 1, Loss: 935.9843, Accuracy: 63.20%\n",
            "Epoch 2, Loss: 569.5058, Accuracy: 78.20%\n",
            "Epoch 3, Loss: 490.6692, Accuracy: 81.09%\n",
            "Epoch 4, Loss: 436.3489, Accuracy: 83.37%\n",
            "Epoch 5, Loss: 403.8325, Accuracy: 84.64%\n",
            "Training ResNet:\n",
            "Epoch 1, Loss: 340.1113, Accuracy: 86.76%\n",
            "Epoch 2, Loss: 233.2627, Accuracy: 90.84%\n",
            "Epoch 3, Loss: 193.8540, Accuracy: 92.45%\n",
            "Epoch 4, Loss: 167.5208, Accuracy: 93.33%\n",
            "Epoch 5, Loss: 143.9987, Accuracy: 94.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **InceptionNet is Better for Multi-Scale Objects**\n",
        "\n",
        "### üîç 1. **Built-in Multi-Scale Processing**\n",
        "\n",
        "Inception modules **simultaneously process features at multiple spatial scales** using parallel convolutional filters:\n",
        "\n",
        "* **1√ó1 convolutions** capture local and low-complexity features.\n",
        "* **3√ó3 and 5√ó5 convolutions** capture medium and large-scale features.\n",
        "* **Max-pooling** adds robustness to spatial translations.\n",
        "\n",
        "> This design allows the network to **learn features from small logos to large patterns**, ideal for diverse object sizes.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† 2. **Why ResNet Falls Short for Multi-Scale**\n",
        "\n",
        "ResNet processes features in a **single-resolution hierarchy**. While skip connections help in training deep networks by preserving gradients, **it doesn't explicitly model multiple scales** in the same layer.\n",
        "\n",
        "* Good for deep and residual learning.\n",
        "* But less adaptive to size variation within the same layer.\n",
        "\n",
        "---\n",
        "\n",
        "### üìê 3. **Mathematical Insight**\n",
        "\n",
        "Inception = Approximate sparse structure via dense computation.\n",
        "\n",
        "Each Inception block:\n",
        "\n",
        "```python\n",
        "Output = Concat(\n",
        "    Conv1x1(x),\n",
        "    Conv3x3(x),\n",
        "    Conv5x5(x),\n",
        "    MaxPool(x)\n",
        ")\n",
        "```\n",
        "\n",
        "This **concatenation of multi-kernel outputs** enriches feature representation across scales ‚Äî something ResNet layers don‚Äôt inherently do.\n",
        "\n",
        "---\n",
        "\n",
        "### üî¨ 4. **Empirical Evidence (Fashion-MNIST resized to 96√ó96)**\n",
        "\n",
        "| Model        | Accuracy | Performance on Varying Object Sizes  |\n",
        "| ------------ | -------- | ------------------------------------ |\n",
        "| ResNet       | ‚úÖ High   | ‚ö†Ô∏è Struggles with small logo details |\n",
        "| InceptionNet | ‚úÖ High   | ‚úÖ Consistent across object scales    |\n",
        "\n",
        "---\n",
        "\n",
        "## üèÅ Conclusion\n",
        "\n",
        "> **InceptionNet** is more suitable for multi-scale image recognition tasks due to its **parallel convolutions** capturing diverse feature sizes **within the same block**.\n",
        "\n",
        "Would you like a code snippet of a simplified Inception block or a markdown version of this explanation?\n"
      ],
      "metadata": {
        "id": "9MF7VZSfyF6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise 4 Transfer Learning Wildlife"
      ],
      "metadata": {
        "id": "cZbkoRiQy0h-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Load & Modify Pre-Trained ResNet-50"
      ],
      "metadata": {
        "id": "c4YvgkaYy-pp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models\n",
        "\n",
        "# Load pretrained model\n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Freeze all layers\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# Unfreeze last few layers\n",
        "for param in list(model.parameters())[-10:]:  # Adjust depending on memory\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Replace classifier\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Linear(num_ftrs, 5)  # 5 classes: e.g., Lion, Elephant, Tiger, etc."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXAMc5xgzCN0",
        "outputId": "115eb6d6-c609-4195-c20b-03ec50ec3b6e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 97.8M/97.8M [00:00<00:00, 196MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Apply CutMix & Label Smoothing"
      ],
      "metadata": {
        "id": "n_TzWrDEzG35"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cutmix(data, targets, alpha=1.0):\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    rand_index = torch.randperm(data.size(0))\n",
        "    target_a = targets\n",
        "    target_b = targets[rand_index]\n",
        "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
        "    data[:, :, bbx1:bbx2, bby1:bby2] = data[rand_index, :, bbx1:bbx2, bby1:bby2]\n",
        "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size()[-1] * data.size()[-2]))\n",
        "    return data, target_a, target_b, lam"
      ],
      "metadata": {
        "id": "eZkmpyXZzKyO"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "source": [
        "import torch\n",
        "\n",
        "def rand_bbox(size, lam):\n",
        "    W = size[2]\n",
        "    H = size[3]\n",
        "    cut_rat = np.sqrt(1. - lam)\n",
        "    cut_w = np.int(W * cut_rat)\n",
        "    cut_h = np.int(H * cut_rat)\n",
        "\n",
        "    # uniform\n",
        "    cx = np.random.randint(W)\n",
        "    cy = np.random.randint(H)\n",
        "\n",
        "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
        "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
        "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
        "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
        "\n",
        "    return bbx1, bby1, bbx2, bby2"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "IqCarsylz915"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.1 Label Smoothing"
      ],
      "metadata": {
        "id": "WUDwrrozzNv7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LabelSmoothingLoss(nn.Module):\n",
        "    def __init__(self, classes, smoothing=0.1):\n",
        "        super(LabelSmoothingLoss, self).__init__()\n",
        "        self.confidence = 1.0 - smoothing\n",
        "        self.smoothing = smoothing\n",
        "        self.cls = classes\n",
        "        self.log_softmax = nn.LogSoftmax(dim=-1)\n",
        "\n",
        "    def forward(self, x, target):\n",
        "        log_probs = self.log_softmax(x)\n",
        "        true_dist = torch.zeros_like(log_probs)\n",
        "        true_dist.fill_(self.smoothing / (self.cls - 1))\n",
        "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
        "        return torch.mean(torch.sum(-true_dist * log_probs, dim=-1))"
      ],
      "metadata": {
        "id": "zINHkGOwzRtS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Training Loop (Skeleton)"
      ],
      "metadata": {
        "id": "kvEDrR10zXcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "criterion = LabelSmoothingLoss(classes=5, smoothing=0.1)\n",
        "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    for data, targets in train_loader:\n",
        "        # Apply CutMix\n",
        "        data, targets_a, targets_b, lam = cutmix(data, targets)\n",
        "        outputs = model(data)\n",
        "        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "L3oQ1VxSzY3d",
        "outputId": "3a1f7b48-6a1a-4804-a84f-e49d7d21b2bc"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-13109b79db9a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# Apply CutMix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcutmix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlam\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_a\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-325223754fd6>\u001b[0m in \u001b[0;36mcutmix\u001b[0;34m(data, targets, alpha)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtarget_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtarget_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mbbx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbby1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbby2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrand_bbox\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbx1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbbx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbby1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbby2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrand_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbx1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbbx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbby1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbby2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mlam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbbx2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbbx1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbby2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbby1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-bcaad4966cb2>\u001b[0m in \u001b[0;36mrand_bbox\u001b[0;34m(size, lam)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mcut_rat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcut_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcut_rat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mcut_h\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcut_rat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/numpy/__init__.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(attr)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__former_attrs__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__former_attrs__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mattr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m__expired_attributes__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'numpy' has no attribute 'int'.\n`np.int` was a deprecated alias for the builtin `int`. To avoid this error in existing code, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\nThe aliases was originally deprecated in NumPy 1.20; for more details and guidance see the original release note at:\n    https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}